{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim()>2:\n            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n        target = target.view(-1,1)\n\n        logpt = F.log_softmax(input)\n        logpt = logpt.gather(1,target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type()!=input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0,target.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n        if self.size_average: return loss.mean()\n        else: return loss.sum()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T09:12:05.761492Z","iopub.execute_input":"2022-05-29T09:12:05.762218Z","iopub.status.idle":"2022-05-29T09:12:07.811057Z","shell.execute_reply.started":"2022-05-29T09:12:05.762109Z","shell.execute_reply":"2022-05-29T09:12:07.810232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 696969\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:7000]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][7000:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 13\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:12:07.812648Z","iopub.execute_input":"2022-05-29T09:12:07.813085Z","iopub.status.idle":"2022-05-29T09:28:13.395329Z","shell.execute_reply.started":"2022-05-29T09:12:07.813059Z","shell.execute_reply":"2022-05-29T09:28:13.394404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 69696915516\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:7000]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][7000:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 13\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:28:13.397922Z","iopub.execute_input":"2022-05-29T09:28:13.398246Z","iopub.status.idle":"2022-05-29T09:43:43.962152Z","shell.execute_reply.started":"2022-05-29T09:28:13.398219Z","shell.execute_reply":"2022-05-29T09:43:43.961342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 66669999\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:7000]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][7000:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 13\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:43:43.9644Z","iopub.execute_input":"2022-05-29T09:43:43.96622Z","iopub.status.idle":"2022-05-29T09:59:12.302172Z","shell.execute_reply.started":"2022-05-29T09:43:43.966164Z","shell.execute_reply":"2022-05-29T09:59:12.300444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 111111\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:7000]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][7000:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 20\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(pt,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:59:12.305129Z","iopub.execute_input":"2022-05-29T09:59:12.305516Z","iopub.status.idle":"2022-05-29T10:22:37.321913Z","shell.execute_reply.started":"2022-05-29T09:59:12.305479Z","shell.execute_reply":"2022-05-29T10:22:37.321052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 696969\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:5000]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][5000:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 20\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:22:37.323445Z","iopub.execute_input":"2022-05-29T10:22:37.32396Z","iopub.status.idle":"2022-05-29T10:27:57.520401Z","shell.execute_reply.started":"2022-05-29T10:22:37.323922Z","shell.execute_reply":"2022-05-29T10:27:57.519033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 696969\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:7550]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][7550:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 15\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:27:57.521808Z","iopub.status.idle":"2022-05-29T10:27:57.522484Z","shell.execute_reply.started":"2022-05-29T10:27:57.522252Z","shell.execute_reply":"2022-05-29T10:27:57.522276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers\n# pip  install openpyxl\nimport torch\nfrom transformers import BertTokenizer\nfrom IPython.display import clear_output\nimport os\nimport pandas as pd\nimport re\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertForSequenceClassification\nimport gc\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom copy import deepcopy\n\nPRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndf_train = pd.read_csv('../input/8888888888888/train.csv')\n\ndf_train = df_train.loc[:, ['text', 'target']]\ndf_train.columns = ['text_a', 'label']\n\nseed = 696969\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'text_a'])\nrandom.seed(seed)\nrandom.shuffle(df_train.loc[:,'label'])\ndf_train=df_train.reset_index()\n\nd={'label':df_train['label'].value_counts().index,'count':df_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_train = df_train.loc[:, ['text_a', 'label']][:5500]\nprint(\"train樣本數：\", len(df_train_train))\ndf_train_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False) \n\nd={'label':df_train_train['label'].value_counts().index,'count':df_train_train['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\ndf_train_test = df_train.loc[:, ['text_a', 'label']][5500:]\nprint(\"test樣本數：\", len(df_train_test))\ndf_train_test.to_csv(\"testt.tsv\", sep=\"\\t\", index=False)\n\nd={'label':df_train_test['label'].value_counts().index,'count':df_train_test['label'].value_counts()}\ndf_cat=pd.DataFrame(data=d).reset_index(drop=True)\nprint(df_cat)\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, mode, tokenizer):\n        assert mode in [\"train\", \"testt\", \"submit\"] \n        self.mode = mode\n        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.label_map = {0:0,1:1}\n        self.tokenizer = tokenizer  \n\n    def __getitem__(self, idx):# 定義回傳一筆訓練 / 測試數據的函式\n        if self.mode == \"submit\":\n            text_a, id= self.df.iloc[idx,:].values\n            label_tensor = None\n        else:\n            text_a,label = self.df.iloc[idx, :].values\n            label_id = self.label_map[label]\n            label_tensor = torch.tensor(label_id)\n         \n        word_pieces = [\"[CLS]\"]\n        tokens_a = self.tokenizer.tokenize(text_a)\n        word_pieces += tokens_a + [\"[SEP]\"]\n        len_a = len(word_pieces)\n        \n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n        \n        segments_tensor = torch.tensor([0] * len_a ,dtype=torch.long)\n        return (tokens_tensor, segments_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    if samples[0][2] is not None:\n        label_ids = torch.stack([s[2] for s in samples])\n    else:\n        label_ids = None\n    \n    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\ndef get_predictions(model, dataloader, compute_acc=False):\n    model.eval()\n    predictions = None\n    correct = 0\n    total = 0\n    labelss = None\n    with torch.no_grad():\n        for data in dataloader:\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n            outputs = model(input_ids=tokens_tensors, \n                            token_type_ids=segments_tensors, \n                            attention_mask=masks_tensors)\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            if compute_acc:\n                labels = data[3]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                if labelss is None:\n                    labelss = data[3]\n                else:\n                    labelss = torch.cat((labelss, data[3]))\n                \n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n            \n    if compute_acc:\n        acc = correct / total\n        return f1_score(predictions.data.cpu().numpy(), labelss.data.cpu().numpy()), acc\n    \n    return predictions\n\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel2=model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\nmodel2 = model2.to(device)\n\ntrainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\ntrainloader = DataLoader(trainset, batch_size=32, collate_fn=create_mini_batch,shuffle=True)\n\ntestset = FakeNewsDataset(\"testt\", tokenizer=tokenizer)\ntestloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n\n# model.train() # 訓練模式\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-6) \n\nEPOCHS = 13\nrecord_max=0\nrecord_max_f1_score=0\ne=0\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n\n    for data in trainloader:\n        model.train()\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n\n        optimizer.zero_grad() \n\n        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors,labels=labels)\n        loss=outputs[0]\n        #     loss_fct = FocalLoss()\n        #     loss = loss_fct(outputs[0], labels)     \n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() \n\n    f1_train, acc = get_predictions(model, trainloader, compute_acc=True) \n    f1_test, tacc = get_predictions(model, testloader, compute_acc=True) \n\n    torch.save(deepcopy(model.state_dict()), str(epoch)+\".pt\")\n\n    if(record_max_f1_score<f1_test):\n        e=epoch\n        record_max_f1_score=f1_test\n    if(record_max<tacc):\n        record_max=tacc\n\n    print(\"epoch:\",epoch,\"f1_score:\",f1_test,\"f1_score_max:\",record_max_f1_score,\"\\n\") \n\n# model2 = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(str(e)+\".pt\"))\nmodel.eval()\npt, tacc = get_predictions(model, testloader, compute_acc=True)\nprint(tacc,e)\n\ndf_submit = pd.read_csv(\"../input/8888888888888/test.csv\")\ndf_submit = df_submit.loc[:,['text','id']]\ndf_submit.columns = [\"text_a\",\"id\"]\n\ndf_submit.to_csv(\"submit.tsv\", sep=\"\\t\", index=False)\nprint(df_submit)\n\nsubmitset = FakeNewsDataset(\"submit\", tokenizer=tokenizer)\nsubmitloader = DataLoader(submitset, batch_size=1, collate_fn=create_mini_batch)\n\npredictions = get_predictions(model, submitloader)\n\nindex_map = {v: k for k, v in testset.label_map.items()}\n\nprint(e)\n\ndf = pd.DataFrame({\"target\": predictions.tolist()})\ndf['target'] = df.target.apply(lambda x: index_map[x])\ndf_pred = pd.concat([submitset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\ndf_pred.to_csv('kkkkkk'+str(e)+'.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:27:57.523903Z","iopub.status.idle":"2022-05-29T10:27:57.524558Z","shell.execute_reply.started":"2022-05-29T10:27:57.524333Z","shell.execute_reply":"2022-05-29T10:27:57.524357Z"},"trusted":true},"execution_count":null,"outputs":[]}]}